{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cây quyết định\n",
    "**Cây quyết định (Decision Tree)** là một phương pháp học máy có giám sát không tham số được sử dụng để phân lớp và hồi quy.\n",
    "\n",
    "Mục đích của cây quyết định là tạo ra một mô hình dự đoán kết quả mục tiêu bằng cách học các luật quyết định đơn giản được suy diễn ra từ các đặc trưng dữ liệu.\n",
    "\n",
    "Mỗi tập luật định nghĩa ra một giả thuyết, có thể được biểu diễn bằng một cây quyết định với đường đi xuôi từ gốc đến lá cho ta một luật quyết định. Nút gốc và mỗi nút trên cây là một thuộc tính/ điều kiện kiểm tra, các nhánh đi xuống từ nút ứng với các giá trị có thể của thuộc tính/điều kiện này. Nhãn của các mẫu phù hợp là các nút lá.\n",
    "\n",
    "\n",
    "Hình dưới đây minh họa một cây quyết định của dữ liệu **Titanic** dự đoán khả năng sống sót khi tàu chìm.\n",
    "<img src=\"titanic.png\" style=\"text-align:center; max-height:400px\">\n",
    "\n",
    "**Bài tập:** Mô tả tập luật của cây quyết định trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trả lời**: *Điền đáp án vào đây!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình cây quyết định trong Scikit-learn\n",
    "Trong `Scikit-learn`, mô hình cây quyết định được cài đặt trong gói `tree` với `DecisionTreeClassifier`.\n",
    "\n",
    "**Bài tập:** Import dữ liệu và mô hình cây quyết định từ `Scikit-learn`, sau đó huấn luyện và biểu diễn mô hình thu được sau khi huấn luyện.\n",
    "\n",
    "*Gợi ý:* Sử dụng kiến thức từ bài thực hành trước với mô hình `SVM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Import mô hình và dữ liệu cần thiết từ thư viện\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# TODO: Chia dữ liệu huấn luyện và kiểm tra hợp lý\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# TODO: Huấn luyện mô hình với dữ liệu\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Visualize mô hình vừa được xây dựng với tập dữ liệu kiểm tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các thuật toán xây dựng cây quyết định cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vấn đề cơ bản của bài toán xây dựng cây quyết định là:\n",
    "- Xác định thuộc tính/điều kiện của mỗi nút\n",
    "- Thứ tự các nút\n",
    "\n",
    "Trong bài này, chúng ta sẽ làm quen với 2 thuật toán cơ bản nhất là **ID3** (Iterative Dichotomiser 3) và **C4.5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thuật toán ID3 (Quinlan 1986) chọn thuộc tính tốt nhất của tập huấn luyện được làm nút gốc theo tiêu chuẩn cực đại lượng thu hoạch thông tin (Information Gain). \n",
    "\n",
    "### Entropy\n",
    "Entropy dùng để đo độ không chắc chắn (độ mù mờ của thông tin). Nếu ta tập dữ liệu $D$ có $N$ phần tử, thuộc $C$ lớp và số phần tử mỗi lớp là $N_c$ thì entropy của tập dữ liệu $D$ được tính theo công thức:\n",
    "\n",
    "$$ E(D) = - \\sum_{c=1}^{C} \\frac{N_c}{N}\\log (\\frac{N_c}{N}) = - \\sum_{c=1}^{C}p_c\\log(p_c)$$\n",
    "**Bài tập**: Định nghĩa hàm `entropy(freq)` để tính entropy của phân phối xác suất dữ liệu `freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 2.5764258916820024\n"
     ]
    }
   ],
   "source": [
    "# TODO: Để có thể xây dựng được cây quyết định, việc đầu tiên cần làm là tính \n",
    "#       toán entropy cho dữ liệu với một phân phối cho trước (hoặc được tính\n",
    "#       toán thông qua dữ liệu)\n",
    "#       Định nghĩa hàm entropy(freq) tính toán độ mù mờ của dữ liệu với phân \n",
    "#       phối xác suất freq, là tần suất của mỗi lớp c trong bộ dữ liệu D. Hàm trả về số thực là độ đo entropy tương ứng.\n",
    "import numpy as np\n",
    "def entropy(freq):\n",
    "    ent = (-freq * np.log2(freq)).sum()\n",
    "    return ent\n",
    "\n",
    "freq = np.array([0.2, 0.3, 0.12, 0.18, 0.08, 0.06, 0.06])\n",
    "print(\"Entropy = {}\".format(entropy(freq)))\n",
    "\n",
    "# Kết quả xấp xỉ 2.576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy hai thuộc tính\n",
    "Khi thuộc tính $x_i$ được chọn làm nút, chia tập $D$ thành $K$ nhánh con $D_1, D_2,...,D_k$, số lượng phần tử trong mỗi nốt con kí hiệu là $m_k$. Độ đo entropy  sau phép chia này được tính:\n",
    "$$ E(D,x_i) = \\sum_{k=1}^{K} \\frac{m_k}{N}E(D_k)= \\sum_{k\\in K}P(k)E(D_k) $$\n",
    "\n",
    "**Bài tập:** Tính độ đo entropy khi có thêm một thuộc tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy with iris: 0.7080248798300984\n"
     ]
    }
   ],
   "source": [
    "def entropy_arr(arr):\n",
    "    _, freq = np.unique(arr, return_counts=True)\n",
    "    freq = np.array(freq) / arr.shape[0]\n",
    "    return entropy(freq)\n",
    "\n",
    "def _entropy(data, target, new_attr):\n",
    "    col = data[:, new_attr]\n",
    "    ent_full = entropy_arr(target)\n",
    "    \n",
    "    ent = 0.0\n",
    "    for val in np.unique(col):\n",
    "        Dk = target[col == val]\n",
    "        ent += entropy_arr(Dk) * Dk.shape[0] / data.shape[0]\n",
    "    \n",
    "    return ent\n",
    "\n",
    "# Tính entropy cho dữ liệu hoa cẩm chướng khi chọn độ dài lá để chia\n",
    "data, target = X, y\n",
    "iris_entropy = _entropy(data, target, 0)\n",
    "print(\"Entropy with iris: {}\".format(iris_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Độ thu hoạch thông tin\n",
    "Độ thu hoạch thông tin được tính là độ giảm entropy khi biết thêm một thông tin $x$:\n",
    "$$ Gain(D,x_i) = G(D,x_i)= E(D) - E(D,x_i) $$\n",
    "\n",
    "Thuộc tính nào cho độ mù mờ thông tin (entropy) nhỏ nhất hay có độ thu hoạch thông tin lớn nhất sẽ được chọn làm thuộc tính tại nút.\n",
    "\n",
    "$$ x^* = \\underset{x}{\\arg\\max}G(D,x_i) = \\underset{x}{\\arg\\min}E(D,x_i) $$\n",
    "**Bài tập:** Viết hàm tính độ thu hoạch thông tin khi thử chọn một thuộc tính làm thuộc tính chia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(data, target, new_attr):\n",
    "    col = data[:, new_attr]\n",
    "    ent_full = entropy_arr(target)\n",
    "    ent = _entropy(data, target, new_attr)\n",
    "    return ent_full - ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập:** Với tập dữ liệu hoa cẩm chướng ban đầu, chọn ra thuộc tính chia tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute can be used: 2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dựa vào các hàm đã xây dựng trước đó, chọn ra nút gốc cho cây quyết định\n",
    "#       của hoa cẩm chướng (trả về chỉ số của thuộc tính trong tập dữ liệu)\n",
    "\n",
    "# chọn thuộc tính tốt nhất với thuật toán ID3\n",
    "\n",
    "best_attr = np.argmax([gain(data, target, a) for a in range(data.shape[1])])\n",
    "print(\"Best attribute can be used: {}\".format(best_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5\n",
    "Thuật toán C4.5 được đề xuất năm 1993 bởi Quinlan nhằm khắc phục điểm yếu của thuật toán ID3: áp dụng Tỷ lệ thu hoạch thông tin cực đại (Gain Ratio).\n",
    "\n",
    "Tỷ lệ thu hoạch này phạt các thuộc tính có nhiều giá trị bằng cách thêm vào một hạng tử gọi là `thông tin chia` (Split Information), đại lượng này rất nhạy cảm với việc đánh giá tính rộng và đồng nhất khi chia tách dữ liệu theo giá trị thuộc tính:\n",
    "$$ SplitInformation(D,x_i)=-\\sum_{i=1}^{k} \\frac{\\left|D_i\\right|}{\\left|D\\right|} \\log{\\frac{\\left|D_i\\right|}{\\left|D\\right|}}$$\n",
    "`Split Information` thực tế là entropy của tập dữ liệu `D` ứng với thuộc tính chia `x_i`.\n",
    "\n",
    "Khi đó, tỷ lệ thông tin chia được tính bằng cách chia độ thu hoạch thông tin cho thông tin chia.\n",
    "$$ GainRatio(D, x_i) = \\frac{Gain(D,x_i)}{SplitInformation(D,x_i)} $$\n",
    "\n",
    "**Bài tập:** Hoàn thành hàm `split_infor(...)` tính thông tin chia và hàm `gain_ratio(...)` để tỷ lệ thu hoạch thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Định nghĩa hai hàm split_infor(...) và gain_ratio(...) để cải thiện thuật toán \n",
    "#       ID3 theo ý tưởng của C4.5\n",
    "def split_infor(data, new_attr):\n",
    "    vals, freq = np.unique(data[:, new_attr], return_counts=True)\n",
    "    return entropy(np.array(freq) / data.shape[0])\n",
    "\n",
    "def gain_ratio(data, target, new_attr):\n",
    "    gain_infor = gain(data, target, new_attr)\n",
    "    split_inf = split_infor(data, new_attr)\n",
    "    gain_ratio = gain_infor / split_inf\n",
    "    return gain_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Bài tập:** Dựa trên việc cải thiện thuật toán ID3, chọn lại nút gốc cho cây quyết định với dữ liệu hoa cẩm chướng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute can be used: 3\n"
     ]
    }
   ],
   "source": [
    "ratios = np.array([gain_ratio(data, target, a) for a in range(data.shape[1])])\n",
    "ratios = ratios[~np.isnan(ratios)]\n",
    "best_attr = np.argmax(ratios)\n",
    "print(\"Best attribute can be used: {}\".format(best_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
